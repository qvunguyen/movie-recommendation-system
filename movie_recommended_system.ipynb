{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install required libraries\n",
    "\n",
    "pip install scikit-surprise\n",
    "\n",
    "pip install pandas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Import required libraries and load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries for creating and evaluating recommendation system models using Surprise package \n",
    "import pandas as pd                          \n",
    "from surprise import Dataset, Reader, SVD         \n",
    "from surprise.model_selection import cross_validate   \n",
    "\n",
    "# Load datasets of movies and ratings from files stored in csv format\n",
    "movies = pd.read_csv('data/movies.csv')\n",
    "ratings = pd.read_csv('data/ratings.csv')\n",
    "links = pd.read_csv('data/links.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Prepare the data for collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scikit-surprise library, we'll create a dataset object and split the data into a train and test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Reader class, defining the rating range as 0.5 to 5 \n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "\n",
    "# Load the data from the ratings dataframe and create a Dataset object,\n",
    "# passing the user id, movie id and rating columns as input to the load_from_df() method\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'tmdbId' to integer and drop rows with missing values\n",
    "links = links.dropna(subset=['tmdbId'])\n",
    "links['tmdbId'] = links['tmdbId'].astype(int)\n",
    "\n",
    "# Merge the MovieLens and TMDB datasets using 'movieId' column\n",
    "movies = movies.merge(links, left_on='movieId', right_on='movieId')\n",
    "\n",
    "# Merge the movies dataset with the ratings dataset\n",
    "movies_with_ratings = movies.merge(ratings, on='movieId')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Build a collaborative filtering model\n",
    "\n",
    "We'll use the SVD algorithm to make movie recommendations based on collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.7777  0.7779  0.7773  0.7778  0.7777  0.7777  0.0002  \n",
      "MAE (testset)     0.5866  0.5869  0.5865  0.5869  0.5865  0.5867  0.0002  \n",
      "Fit time          294.01  5609.21 501.17  4167.51 3482.13 2810.81 2087.63 \n",
      "Test time         952.00  4490.50 608.83  3210.54 6241.48 3100.67 2127.65 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.77766929, 0.77790203, 0.77733974, 0.77776941, 0.77765096]),\n",
       " 'test_mae': array([0.5865782 , 0.58685871, 0.58649324, 0.58687172, 0.58646588]),\n",
       " 'fit_time': (294.00930404663086,\n",
       "  5609.209953069687,\n",
       "  501.1718888282776,\n",
       "  4167.514079093933,\n",
       "  3482.1269397735596),\n",
       " 'test_time': (952.0028901100159,\n",
       "  4490.501410961151,\n",
       "  608.8305280208588,\n",
       "  3210.5446338653564,\n",
       "  6241.480406761169)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an object of the SVD algorithm.\n",
    "svd = SVD()\n",
    "\n",
    "# Use the cross_validate function to apply K-fold cross-validation on the data for the svd model.\n",
    "# Here, 5-fold cross-validation is used.\n",
    "# The evaluation metrics used are RMSE(Root Mean Squared Error) and MAE(Mean Absolute Error).\n",
    "# verbose parameter is set to True to show more details during cross-validation.\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Prepare the data for content-based filtering\n",
    "To perform content-based filtering, we need to transform the movie genres into a feature vector using the TF-IDF approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Instantiate a TfidfVectorizer object with 'english' stop words\n",
    "# This will help us remove common English words such as 'the', 'and', etc., from the text data\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Replace missing genre values with an empty string\n",
    "movies['genres'] = movies['genres'].fillna('')\n",
    "\n",
    "# Fit the TfidfVectorizer to the 'genres' column of the movies DataFrame, transforming the text data into a numerical format\n",
    "# This generates a sparse matrix of TF-IDF values for each genre in the dataset\n",
    "tfidf_matrix = tfidf.fit_transform(movies['genres'])\n",
    "\n",
    "# Compute the cosine similarity between each pair of movies using their corresponding TF-IDF vectors\n",
    "# This will give us a measure of how similar the movies are based on their genres\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Create a function for content-based recommendations\n",
    "This function takes a movie title as input and returns the top n similar movies based on the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas Series with movie titles as the index and their corresponding DataFrame index as values\n",
    "indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n",
    "\n",
    "# Define a function for generating content-based recommendations\n",
    "def content_based_recommendations(title, n=10):\n",
    "    # Obtain the index of the movie that matches the provided title\n",
    "    index = indices[title]\n",
    "    \n",
    "    # Create a list of tuples containing the index and cosine similarity score for each movie\n",
    "    sim_scores = list(enumerate(cosine_sim[index]))\n",
    "    \n",
    "    # Sort the list of similarity scores in descending order, so the highest scores come first\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select the top 'n' highest scoring movies, excluding the first one (itself)\n",
    "    sim_scores = sim_scores[1:n+1]\n",
    "    \n",
    "    # Extract the indices of the selected movies\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the titles of the selected movies using their indices\n",
    "    return movies['title'].iloc[movie_indices]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Combine collaborative and content-based filtering\n",
    "Create a hybrid recommendation function that takes a user ID and movie title as input, and outputs a list of top n recommended movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a hybrid recommendation function that combines content-based and collaborative filtering approaches\n",
    "def hybrid_recommendations(user_id, title, n=10):\n",
    "    # Obtain content-based recommendations for the given title and convert them to a DataFrame\n",
    "    content_based = content_based_recommendations(title, n).to_frame()\n",
    "    content_based.columns = ['title']\n",
    "    \n",
    "    # Merge the content-based recommendations with the movies_with_ratings DataFrame on the 'title' column\n",
    "    content_based = content_based.merge(movies_with_ratings, on='title')\n",
    "    \n",
    "    # Remove duplicate movies, keeping only the first occurrence\n",
    "    content_based = content_based.drop_duplicates(subset=['title'], keep='first')\n",
    "    \n",
    "    # Calculate the estimated rating for each recommended movie using the SVD model and the provided user_id\n",
    "    content_based['est'] = content_based['movieId'].apply(lambda x: svd.predict(user_id, x).est)\n",
    "    \n",
    "    # Sort the movies by their estimated ratings in descending order\n",
    "    content_based = content_based.sort_values('est', ascending=False)\n",
    "    \n",
    "    # Return the top 'n' movie titles with the highest estimated ratings\n",
    "    return content_based.head(n)['title']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Testing recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for User 1 who likes 'Toy Story (1995)':\n",
      "1. Traffic (2000)\n",
      "2. Mighty, The (1998)\n",
      "3. Godzilla vs. Destroyah (Gojira vs. Desutoroiâ) (1995) \n",
      "4. Daddy Long Legs (1919)\n",
      "5. Stagecoach (1966)\n",
      "6. Down in the Valley (2005)\n",
      "7. Hell Up in Harlem (1973)\n",
      "8. Hatchet for the Honeymoon (Rosso segno della follia, Il) (1970)\n",
      "9. Airborne (1993)\n",
      "10. The Alphabet Killer (2008)\n"
     ]
    }
   ],
   "source": [
    "# Define the user ID, movie title, and the number of recommendations to generate\n",
    "user_id = 1\n",
    "title = 'Toy Story (1995)'\n",
    "n = 10\n",
    "\n",
    "# Call the hybrid_recommendations function to generate 'n' recommendations for the user based on the provided movie title\n",
    "recommendations = hybrid_recommendations(user_id, title, n)\n",
    "\n",
    "# Convert the recommendations to a list\n",
    "recommendations_list = recommendations.tolist()\n",
    "\n",
    "# Print the top 'n' recommendations for the user, showing which movie they are based on\n",
    "print(f\"Top {n} recommendations for User {user_id} who likes '{title}':\")\n",
    "\n",
    "# Iterate through the recommendations list and print each movie title with its corresponding rank\n",
    "for i, movie_title in enumerate(recommendations_list, start=1):\n",
    "    print(f\"{i}. {movie_title}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
